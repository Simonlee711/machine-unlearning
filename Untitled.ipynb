{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f79ccf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/simonlee711/miniconda3/envs/ml/lib/python3.10/site-packages (from lightgbm) (2.0.1)\n",
      "Requirement already satisfied: scipy in /home/simonlee711/miniconda3/envs/ml/lib/python3.10/site-packages (from lightgbm) (1.15.1)\n",
      "Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install openml xgboost scikit-learn \n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eabc732-0d1e-4519-be2b-ab1b2e8c6eec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 14:26:11,391 - INFO - Fetching all active binary classification datasets from OpenML with <1000 instances...\n",
      "2025-01-23 14:26:11,394 - INFO - Starting [get] request for the URL https://www.openml.org/api/v1/xml/data/list/number_classes/2/limit/10000/offset/0\n",
      "2025-01-23 14:26:13,048 - INFO - 1.6534159s taken for [get] request for the URL https://www.openml.org/api/v1/xml/data/list/number_classes/2/limit/10000/offset/0\n",
      "2025-01-23 14:26:13,291 - INFO - Available columns in the dataset list: ['did', 'name', 'version', 'uploader', 'status', 'format', 'MajorityClassSize', 'MaxNominalAttDistinctValues', 'MinorityClassSize', 'NumberOfClasses', 'NumberOfFeatures', 'NumberOfInstances', 'NumberOfInstancesWithMissingValues', 'NumberOfMissingValues', 'NumberOfNumericFeatures', 'NumberOfSymbolicFeatures']\n",
      "2025-01-23 14:26:13,294 - WARNING - Warning: 'default_target_attribute' column not found or missing. Skipping this filter.\n",
      "Processing datasets: 100%|██████████| 514/514 [00:00<00:00, 22571.51it/s]\n",
      "2025-01-23 14:26:13,321 - INFO - Total binary classification datasets after filtering: 514\n",
      "2025-01-23 14:26:13,323 - INFO - Fetched 514 binary classification datasets.\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]2025-01-23 14:26:13,326 - INFO - \n",
      "Processing Dataset ID: 4, Name: labor\n",
      "2025-01-23 14:26:13,354 - INFO - pickle write labor\n",
      "2025-01-23 14:26:13,363 - INFO - Training the original model...\n",
      "Training Trees: 100%|██████████| 10/10 [00:01<00:00,  7.01it/s]\n",
      "2025-01-23 14:26:14,821 - INFO - Original Model - Accuracy: 0.9167, Precision: 0.8889, Recall: 1.0000, F1: 0.9412, Log Loss: 0.3614\n",
      "2025-01-23 14:26:14,822 - INFO - Removing 2 samples from training data.\n",
      "2025-01-23 14:26:14,823 - INFO - Starting unlearning of 2 samples.\n",
      "2025-01-23 14:26:14,824 - INFO - Recomputing predictions after unlearning.\n",
      "2025-01-23 14:26:14,842 - INFO - Unlearning process completed.\n",
      "2025-01-23 14:26:14,857 - INFO - Unlearned Model - Accuracy: 0.9167, Precision: 0.8889, Recall: 1.0000, F1: 0.9412, Log Loss: 0.3614\n",
      "2025-01-23 14:26:14,858 - INFO - Retraining the baseline model without the removed samples...\n",
      "Training Trees: 100%|██████████| 10/10 [00:01<00:00,  7.15it/s]\n",
      "2025-01-23 14:26:16,289 - INFO - Retrained Model - Accuracy: 0.7500, Precision: 0.7273, Recall: 1.0000, F1: 0.8421, Log Loss: 0.3612\n",
      "2025-01-23 14:26:16,290 - INFO - Comparison between Unlearned Model and Retrained Model:\n",
      "2025-01-23 14:26:16,291 - INFO - Accuracy Difference: 0.1667\n",
      "2025-01-23 14:26:16,292 - INFO - Precision Difference: 0.1616\n",
      "2025-01-23 14:26:16,293 - INFO - Recall Difference: 0.0000\n",
      "2025-01-23 14:26:16,294 - INFO - F1 Score Difference: 0.0991\n",
      "2025-01-23 14:26:16,295 - INFO - Log Loss Difference: 0.0001\n",
      " 20%|██        | 1/5 [00:02<00:11,  2.97s/it]2025-01-23 14:26:16,297 - INFO - \n",
      "Processing Dataset ID: 13, Name: breast-cancer\n",
      "2025-01-23 14:26:16,307 - INFO - pickle write breast-cancer\n",
      "2025-01-23 14:26:16,317 - INFO - Training the original model...\n",
      "Training Trees: 100%|██████████| 10/10 [00:00<00:00, 11.03it/s]\n",
      "2025-01-23 14:26:17,258 - INFO - Original Model - Accuracy: 0.7586, Precision: 1.0000, Recall: 0.2222, F1: 0.3636, Log Loss: 0.5354\n",
      "2025-01-23 14:26:17,260 - INFO - Removing 11 samples from training data.\n",
      "2025-01-23 14:26:17,261 - INFO - Starting unlearning of 11 samples.\n",
      "2025-01-23 14:26:17,262 - INFO - Recomputing predictions after unlearning.\n",
      "2025-01-23 14:26:17,344 - INFO - Unlearning process completed.\n",
      "2025-01-23 14:26:17,373 - INFO - Unlearned Model - Accuracy: 0.7586, Precision: 1.0000, Recall: 0.2222, F1: 0.3636, Log Loss: 0.5354\n",
      "2025-01-23 14:26:17,375 - INFO - Retraining the baseline model without the removed samples...\n",
      "Training Trees: 100%|██████████| 10/10 [00:00<00:00, 11.00it/s]\n",
      "2025-01-23 14:26:18,326 - INFO - Retrained Model - Accuracy: 0.7414, Precision: 0.8000, Recall: 0.2222, F1: 0.3478, Log Loss: 0.5417\n",
      "2025-01-23 14:26:18,327 - INFO - Comparison between Unlearned Model and Retrained Model:\n",
      "2025-01-23 14:26:18,328 - INFO - Accuracy Difference: 0.0172\n",
      "2025-01-23 14:26:18,329 - INFO - Precision Difference: 0.2000\n",
      "2025-01-23 14:26:18,330 - INFO - Recall Difference: 0.0000\n",
      "2025-01-23 14:26:18,331 - INFO - F1 Score Difference: 0.0158\n",
      "2025-01-23 14:26:18,333 - INFO - Log Loss Difference: 0.0064\n",
      " 40%|████      | 2/5 [00:05<00:07,  2.42s/it]2025-01-23 14:26:18,335 - INFO - \n",
      "Processing Dataset ID: 15, Name: breast-w\n",
      "2025-01-23 14:26:18,348 - INFO - pickle write breast-w\n",
      "2025-01-23 14:26:18,354 - INFO - Training the original model...\n",
      "Training Trees: 100%|██████████| 10/10 [00:02<00:00,  4.58it/s]\n",
      "2025-01-23 14:26:20,603 - INFO - Original Model - Accuracy: 0.9571, Precision: 0.9535, Recall: 0.9111, F1: 0.9318, Log Loss: 0.2477\n",
      "2025-01-23 14:26:20,604 - INFO - Removing 27 samples from training data.\n",
      "2025-01-23 14:26:20,605 - INFO - Starting unlearning of 27 samples.\n",
      "2025-01-23 14:26:20,606 - INFO - Recomputing predictions after unlearning.\n",
      "2025-01-23 14:26:20,801 - INFO - Unlearning process completed.\n",
      "2025-01-23 14:26:20,860 - INFO - Unlearned Model - Accuracy: 0.9571, Precision: 0.9535, Recall: 0.9111, F1: 0.9318, Log Loss: 0.2477\n",
      "2025-01-23 14:26:20,861 - INFO - Retraining the baseline model without the removed samples...\n",
      "Training Trees: 100%|██████████| 10/10 [00:02<00:00,  4.52it/s]\n",
      "2025-01-23 14:26:23,141 - INFO - Retrained Model - Accuracy: 0.9429, Precision: 0.9111, Recall: 0.9111, F1: 0.9111, Log Loss: 0.2555\n",
      "2025-01-23 14:26:23,143 - INFO - Comparison between Unlearned Model and Retrained Model:\n",
      "2025-01-23 14:26:23,144 - INFO - Accuracy Difference: 0.0143\n",
      "2025-01-23 14:26:23,145 - INFO - Precision Difference: 0.0424\n",
      "2025-01-23 14:26:23,146 - INFO - Recall Difference: 0.0000\n",
      "2025-01-23 14:26:23,147 - INFO - F1 Score Difference: 0.0207\n",
      "2025-01-23 14:26:23,148 - INFO - Log Loss Difference: 0.0078\n",
      " 60%|██████    | 3/5 [00:09<00:07,  3.51s/it]2025-01-23 14:26:23,150 - INFO - \n",
      "Processing Dataset ID: 25, Name: colic\n",
      "2025-01-23 14:26:23,177 - INFO - pickle write colic\n",
      "2025-01-23 14:26:23,181 - INFO - Going to remove the following attributes: ['Hospital_Number']\n",
      "2025-01-23 14:26:23,197 - INFO - Training the original model...\n",
      "Training Trees: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "2025-01-23 14:26:32,551 - INFO - Original Model - Accuracy: 0.8649, Precision: 0.8636, Recall: 0.7308, F1: 0.7917, Log Loss: 0.4130\n",
      "2025-01-23 14:26:32,552 - INFO - Removing 14 samples from training data.\n",
      "2025-01-23 14:26:32,553 - INFO - Starting unlearning of 14 samples.\n",
      "2025-01-23 14:26:32,554 - INFO - Recomputing predictions after unlearning.\n",
      "2025-01-23 14:26:32,663 - INFO - Unlearning process completed.\n",
      "2025-01-23 14:26:32,703 - INFO - Unlearned Model - Accuracy: 0.8649, Precision: 0.8636, Recall: 0.7308, F1: 0.7917, Log Loss: 0.4130\n",
      "2025-01-23 14:26:32,704 - INFO - Retraining the baseline model without the removed samples...\n",
      "Training Trees: 100%|██████████| 10/10 [00:08<00:00,  1.11it/s]\n",
      "2025-01-23 14:26:41,745 - INFO - Retrained Model - Accuracy: 0.8649, Precision: 0.8636, Recall: 0.7308, F1: 0.7917, Log Loss: 0.4334\n",
      "2025-01-23 14:26:41,746 - INFO - Comparison between Unlearned Model and Retrained Model:\n",
      "2025-01-23 14:26:41,747 - INFO - Accuracy Difference: 0.0000\n",
      "2025-01-23 14:26:41,748 - INFO - Precision Difference: 0.0000\n",
      "2025-01-23 14:26:41,749 - INFO - Recall Difference: 0.0000\n",
      "2025-01-23 14:26:41,750 - INFO - F1 Score Difference: 0.0000\n",
      "2025-01-23 14:26:41,751 - INFO - Log Loss Difference: 0.0204\n",
      " 80%|████████  | 4/5 [00:28<00:09,  9.47s/it]2025-01-23 14:26:41,753 - INFO - \n",
      "Processing Dataset ID: 27, Name: colic\n",
      "2025-01-23 14:26:41,770 - INFO - pickle write colic\n",
      "2025-01-23 14:26:41,783 - INFO - Training the original model...\n",
      "Training Trees: 100%|██████████| 10/10 [00:08<00:00,  1.22it/s]\n",
      "2025-01-23 14:26:50,016 - INFO - Original Model - Accuracy: 0.8649, Precision: 0.8800, Recall: 0.9167, F1: 0.8980, Log Loss: 0.4198\n",
      "2025-01-23 14:26:50,017 - INFO - Removing 14 samples from training data.\n",
      "2025-01-23 14:26:50,018 - INFO - Starting unlearning of 14 samples.\n",
      "2025-01-23 14:26:50,019 - INFO - Recomputing predictions after unlearning.\n",
      "2025-01-23 14:26:50,125 - INFO - Unlearning process completed.\n",
      "2025-01-23 14:26:50,161 - INFO - Unlearned Model - Accuracy: 0.8649, Precision: 0.8800, Recall: 0.9167, F1: 0.8980, Log Loss: 0.4198\n",
      "2025-01-23 14:26:50,163 - INFO - Retraining the baseline model without the removed samples...\n",
      "Training Trees: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s]\n",
      "2025-01-23 14:26:58,143 - INFO - Retrained Model - Accuracy: 0.8649, Precision: 0.8800, Recall: 0.9167, F1: 0.8980, Log Loss: 0.4172\n",
      "2025-01-23 14:26:58,144 - INFO - Comparison between Unlearned Model and Retrained Model:\n",
      "2025-01-23 14:26:58,145 - INFO - Accuracy Difference: 0.0000\n",
      "2025-01-23 14:26:58,146 - INFO - Precision Difference: 0.0000\n",
      "2025-01-23 14:26:58,147 - INFO - Recall Difference: 0.0000\n",
      "2025-01-23 14:26:58,148 - INFO - F1 Score Difference: 0.0000\n",
      "2025-01-23 14:26:58,149 - INFO - Log Loss Difference: 0.0026\n",
      "100%|██████████| 5/5 [00:44<00:00,  8.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import random\n",
    "\n",
    "# Configure logging for better visibility\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "########################################\n",
    "# 1. Fetch all binary classification datasets #\n",
    "#    with < 1000 samples from OpenML     #\n",
    "########################################\n",
    "\n",
    "def fetch_binary_classification_datasets_under_1000():\n",
    "    logging.info(\"Fetching all active binary classification datasets from OpenML with <1000 instances...\")\n",
    "    # Fetch datasets with binary classification (number_classes=2) and fewer than 1000 instances\n",
    "    all_ds = openml.datasets.list_datasets(\n",
    "        output_format='dataframe',\n",
    "        number_classes=2  # Binary classification\n",
    "    )\n",
    "    df = all_ds\n",
    "    logging.info(f\"Available columns in the dataset list: {df.columns.tolist()}\")\n",
    "\n",
    "    # Filter datasets based on the number of instances\n",
    "    if 'NumberOfInstances' in df.columns:\n",
    "        df = df[df['NumberOfInstances'] < 1000]\n",
    "    else:\n",
    "        logging.warning(\"Warning: 'NumberOfInstances' column not found. Skipping this filter.\")\n",
    "\n",
    "    # Ensure the target attribute is present\n",
    "    target_col = 'default_target_attribute' if 'default_target_attribute' in df.columns else None\n",
    "    if target_col and target_col in df.columns:\n",
    "        df = df.dropna(subset=[target_col])\n",
    "    else:\n",
    "        logging.warning(\"Warning: 'default_target_attribute' column not found or missing. Skipping this filter.\")\n",
    "\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing datasets\"):\n",
    "        did = row['did']\n",
    "        name = row['name']\n",
    "        results.append((did, name))\n",
    "    logging.info(f\"Total binary classification datasets after filtering: {len(results)}\")\n",
    "    return results\n",
    "\n",
    "###############################################################\n",
    "# 2. Helper to load a dataset and ensure it's binary only.    #\n",
    "###############################################################\n",
    "\n",
    "def load_and_prepare_dataset(did):\n",
    "    try:\n",
    "        ds = openml.datasets.get_dataset(did)\n",
    "        name = ds.name\n",
    "        target_name = ds.default_target_attribute\n",
    "        if target_name is None:\n",
    "            logging.warning(f\"Dataset {did} has no default target attribute. Skipping.\")\n",
    "            return None\n",
    "        X, y, _, _ = ds.get_data(\n",
    "            target=target_name,\n",
    "            dataset_format='dataframe'\n",
    "        )\n",
    "        n_unique = y.nunique()\n",
    "        if n_unique != 2:\n",
    "            logging.info(f\"Dataset {did} is not binary. It has {n_unique} classes. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        # Encode target if necessary\n",
    "        if not pd.api.types.is_numeric_dtype(y):\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "            y = pd.Series(y)\n",
    "        else:\n",
    "            unique_vals = sorted(y.unique())\n",
    "            if set(unique_vals) != {0, 1}:\n",
    "                le = LabelEncoder()\n",
    "                y = le.fit_transform(y)\n",
    "                y = pd.Series(y)\n",
    "            else:\n",
    "                if not isinstance(y, pd.Series):\n",
    "                    y = pd.Series(y)\n",
    "\n",
    "        # Convert object and category dtypes to numerical codes\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "\n",
    "        # Reset index to ensure consistent indexing\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "\n",
    "        return X, y, name\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading dataset {did}: {e}\")\n",
    "        return None\n",
    "\n",
    "###############################################################\n",
    "# 3. Custom Gradient Boosted Trees with Unlearning            #\n",
    "###############################################################\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, min_samples_split=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None  # Will hold the tree structure\n",
    "\n",
    "    def fit(self, X, y, gradients, hessians):\n",
    "        self.features = X.columns\n",
    "        self.tree = self._build_tree(X, y, gradients, hessians, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, gradients, hessians, depth):\n",
    "        num_samples = X.shape[0]\n",
    "        if depth >= self.max_depth or num_samples < self.min_samples_split:\n",
    "            # Leaf node\n",
    "            grad_sum = np.sum(gradients)\n",
    "            hess_sum = np.sum(hessians)\n",
    "            leaf_weight = -grad_sum / (hess_sum + 1e-6)  # Avoid division by zero\n",
    "            return {\n",
    "                'type': 'leaf',\n",
    "                'weight': leaf_weight,\n",
    "                'grad_sum': grad_sum,\n",
    "                'hess_sum': hess_sum,\n",
    "                'samples': X.index.tolist()\n",
    "            }\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold, best_gain = self._find_best_split(X, gradients, hessians)\n",
    "        if best_gain is None or best_gain <= 0:\n",
    "            # Cannot find a better split\n",
    "            grad_sum = np.sum(gradients)\n",
    "            hess_sum = np.sum(hessians)\n",
    "            leaf_weight = -grad_sum / (hess_sum + 1e-6)\n",
    "            return {\n",
    "                'type': 'leaf',\n",
    "                'weight': leaf_weight,\n",
    "                'grad_sum': grad_sum,\n",
    "                'hess_sum': hess_sum,\n",
    "                'samples': X.index.tolist()\n",
    "            }\n",
    "\n",
    "        # Split the dataset\n",
    "        left_indices = X[best_feature] <= best_threshold\n",
    "        right_indices = X[best_feature] > best_threshold\n",
    "\n",
    "        # Recursively build left and right subtrees\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], gradients[left_indices], hessians[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], gradients[right_indices], hessians[right_indices], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature': best_feature,\n",
    "            'threshold': best_threshold,\n",
    "            'left': left_subtree,\n",
    "            'right': right_subtree\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, gradients, hessians):\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in self.features:\n",
    "            unique_values = X[feature].unique()\n",
    "            if len(unique_values) <= 1:\n",
    "                continue\n",
    "            thresholds = unique_values[:-1]  # Avoid the last value to prevent empty split\n",
    "            for threshold in thresholds:\n",
    "                left = X[feature] <= threshold\n",
    "                right = X[feature] > threshold\n",
    "\n",
    "                if np.sum(left) == 0 or np.sum(right) == 0:\n",
    "                    continue\n",
    "\n",
    "                left_grad = np.sum(gradients[left])\n",
    "                right_grad = np.sum(gradients[right])\n",
    "                left_hess = np.sum(hessians[left])\n",
    "                right_hess = np.sum(hessians[right])\n",
    "\n",
    "                gain = self._calculate_gain(left_grad, left_hess) + self._calculate_gain(right_grad, right_hess) - self._calculate_gain(np.sum(gradients), np.sum(hessians))\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_gain > 0:\n",
    "            return best_feature, best_threshold, best_gain\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    def _calculate_gain(self, grad_sum, hess_sum):\n",
    "        return (grad_sum ** 2) / (hess_sum + 1e-6)  # Regularization can be added here\n",
    "\n",
    "    def predict_single(self, x, node=None):\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['weight']\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self.predict_single(x, node['left'])\n",
    "        else:\n",
    "            return self.predict_single(x, node['right'])\n",
    "\n",
    "    def get_leaf_mappings(self, X):\n",
    "        \"\"\"\n",
    "        Returns a mapping from sample index to leaf node in this tree.\n",
    "        \"\"\"\n",
    "        mapping = {}\n",
    "        for idx, row in X.iterrows():\n",
    "            weight, samples = self._get_leaf_info(row, self.tree)\n",
    "            mapping[idx] = {'weight': weight, 'samples': samples}\n",
    "        return mapping\n",
    "\n",
    "    def _get_leaf_info(self, x, node):\n",
    "        if node['type'] == 'leaf':\n",
    "            return node['weight'], node['samples']\n",
    "        if x[node['feature']] <= node['threshold']:\n",
    "            return self._get_leaf_info(x, node['left'])\n",
    "        else:\n",
    "            return self._get_leaf_info(x, node['right'])\n",
    "\n",
    "    def update_leaf(self, samples_to_remove, gradients_to_remove, hessians_to_remove):\n",
    "        \"\"\"\n",
    "        Update the leaf weights by removing the contributions of certain samples.\n",
    "        \"\"\"\n",
    "        # Find the leaf containing the samples\n",
    "        leaf = self._find_leaf(self.tree, samples_to_remove)\n",
    "        if leaf is not None:\n",
    "            # Update gradients and hessians\n",
    "            leaf['grad_sum'] -= gradients_to_remove\n",
    "            leaf['hess_sum'] -= hessians_to_remove\n",
    "\n",
    "            # Recompute the leaf weight\n",
    "            if leaf['hess_sum'] > 0:\n",
    "                leaf['weight'] = -leaf['grad_sum'] / (leaf['hess_sum'] + 1e-6)\n",
    "            else:\n",
    "                leaf['weight'] = 0.0  # Default weight if hess_sum is zero\n",
    "\n",
    "    def _find_leaf(self, node, samples_to_remove):\n",
    "        \"\"\"\n",
    "        Recursively find the leaf node containing any of the samples to remove.\n",
    "        \"\"\"\n",
    "        if node['type'] == 'leaf':\n",
    "            if any(sample in node['samples'] for sample in samples_to_remove):\n",
    "                return node\n",
    "            else:\n",
    "                return None\n",
    "        # Check left subtree\n",
    "        left_samples = [s for s in samples_to_remove if s in node['left']['samples']]\n",
    "        if left_samples:\n",
    "            leaf = self._find_leaf(node['left'], left_samples)\n",
    "            if leaf:\n",
    "                return leaf\n",
    "        # Check right subtree\n",
    "        right_samples = [s for s in samples_to_remove if s in node['right']['samples']]\n",
    "        if right_samples:\n",
    "            leaf = self._find_leaf(node['right'], right_samples)\n",
    "            if leaf:\n",
    "                return leaf\n",
    "        return None\n",
    "\n",
    "class GradientBoostedTrees:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, min_samples_split=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "        self.init_score = 0.0\n",
    "        # To store gradients and hessians for each sample and tree\n",
    "        self.sample_gradients = []  # List of numpy arrays\n",
    "        self.sample_hessians = []   # List of numpy arrays\n",
    "        self.F = None               # Raw predictions before sigmoid\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with log(odds)\n",
    "        pos = np.sum(y)\n",
    "        neg = len(y) - pos\n",
    "        self.init_score = np.log((pos + 1) / (neg + 1e-6))\n",
    "        self.F = np.full(len(y), self.init_score)\n",
    "\n",
    "        for i in tqdm(range(self.n_estimators), desc=\"Training Trees\"):\n",
    "            # Compute gradients and hessians\n",
    "            p = self.sigmoid(self.F)\n",
    "            g = p - y  # Gradient\n",
    "            h = p * (1 - p)  # Hessian\n",
    "\n",
    "            # Store gradients and hessians\n",
    "            self.sample_gradients.append(g.copy())\n",
    "            self.sample_hessians.append(h.copy())\n",
    "\n",
    "            # Fit a decision tree to the gradients\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X, y, g, h)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Update F\n",
    "            for idx, row in X.iterrows():\n",
    "                self.F[idx] += self.learning_rate * tree.predict_single(row)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        F = np.full(X.shape[0], self.init_score)\n",
    "        for tree in self.trees:\n",
    "            for idx, row in X.iterrows():\n",
    "                F[idx] += self.learning_rate * tree.predict_single(row)\n",
    "        proba = self.sigmoid(F)\n",
    "        return proba\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "    def get_leaf_mappings(self, X):\n",
    "        \"\"\"\n",
    "        Get leaf mappings for all trees.\n",
    "        Returns a list of dictionaries for each tree.\n",
    "        Each dictionary maps sample index to leaf node information.\n",
    "        \"\"\"\n",
    "        mappings = []\n",
    "        for tree in self.trees:\n",
    "            mapping = tree.get_leaf_mappings(X)\n",
    "            mappings.append(mapping)\n",
    "        return mappings\n",
    "\n",
    "    def unlearn(self, X, y, indices_to_remove):\n",
    "        \"\"\"\n",
    "        Perform unlearning by removing specific samples.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Starting unlearning of {len(indices_to_remove)} samples.\")\n",
    "\n",
    "        for tree_idx, tree in enumerate(self.trees):\n",
    "            # Get the gradients and hessians for this tree\n",
    "            g = self.sample_gradients[tree_idx]\n",
    "            h = self.sample_hessians[tree_idx]\n",
    "\n",
    "            # Identify the samples to remove in this tree\n",
    "            # Ensure indices are within bounds\n",
    "            samples_in_tree = [idx for idx in indices_to_remove if idx < len(g)]\n",
    "            # Further filter to ensure the sample is in a leaf\n",
    "            samples_in_tree = [idx for idx in samples_in_tree if tree.tree and tree.tree.get('type') == 'leaf' and idx in tree.tree.get('samples', [])]\n",
    "            if not samples_in_tree:\n",
    "                continue  # No samples to remove in this tree\n",
    "\n",
    "            # Aggregate gradients and hessians to remove\n",
    "            g_remove = np.sum(g[samples_in_tree])\n",
    "            h_remove = np.sum(h[samples_in_tree])\n",
    "\n",
    "            # Update the leaf containing these samples\n",
    "            tree.update_leaf(samples_in_tree, g_remove, h_remove)\n",
    "\n",
    "            # Zero out the gradients and hessians for the removed samples\n",
    "            self.sample_gradients[tree_idx][samples_in_tree] = 0.0\n",
    "            self.sample_hessians[tree_idx][samples_in_tree] = 0.0\n",
    "\n",
    "        # Recompute predictions F after unlearning\n",
    "        logging.info(\"Recomputing predictions after unlearning.\")\n",
    "        self.F = np.full(len(y), self.init_score)\n",
    "        for tree in self.trees:\n",
    "            for idx, row in X.iterrows():\n",
    "                self.F[idx] += self.learning_rate * tree.predict_single(row)\n",
    "\n",
    "        logging.info(\"Unlearning process completed.\")\n",
    "\n",
    "###############################################################\n",
    "# 4. Evaluation Functions                                     #\n",
    "###############################################################\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    preds = model.predict_proba(X_test)\n",
    "    preds_binary = (preds >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_test, preds_binary)\n",
    "    prec = precision_score(y_test, preds_binary, zero_division=0)\n",
    "    rec = recall_score(y_test, preds_binary, zero_division=0)\n",
    "    f1 = f1_score(y_test, preds_binary, zero_division=0)\n",
    "    ll = log_loss(y_test, preds)\n",
    "    return acc, prec, rec, f1, ll\n",
    "\n",
    "def main():\n",
    "    # Step 1: Fetch datasets\n",
    "    datasets = fetch_binary_classification_datasets_under_1000()\n",
    "    logging.info(f\"Fetched {len(datasets)} binary classification datasets.\")\n",
    "\n",
    "    if len(datasets) == 0:\n",
    "        logging.error(\"No datasets found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # For demonstration, we'll process only the first 5 datasets\n",
    "    for did, name in tqdm(datasets[:5]):\n",
    "        logging.info(f\"\\nProcessing Dataset ID: {did}, Name: {name}\")\n",
    "        loaded = load_and_prepare_dataset(did)\n",
    "        if loaded is None:\n",
    "            continue\n",
    "        X, y, dataset_name = loaded\n",
    "\n",
    "        # Split into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Reset indices to ensure alignment\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        y_train = y_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        # Initialize and train the model\n",
    "        logging.info(\"Training the original model...\")\n",
    "        gbm = GradientBoostedTrees(n_estimators=10, learning_rate=0.1, max_depth=3, min_samples_split=10)\n",
    "        gbm.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the model before unlearning\n",
    "        acc, prec, rec, f1, ll = evaluate_model(gbm, X_test, y_test)\n",
    "        logging.info(f\"Original Model - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, Log Loss: {ll:.4f}\")\n",
    "\n",
    "        # Select random indices to remove (e.g., 5% of the training data)\n",
    "        num_remove = max(1, int(0.05 * len(X_train)))\n",
    "        indices_to_remove = random.sample(range(len(X_train)), num_remove)\n",
    "        logging.info(f\"Removing {num_remove} samples from training data.\")\n",
    "\n",
    "        # Perform unlearning\n",
    "        gbm.unlearn(X_train, y_train, indices_to_remove)\n",
    "\n",
    "        # Evaluate the model after unlearning\n",
    "        acc_unlearn, prec_unlearn, rec_unlearn, f1_unlearn, ll_unlearn = evaluate_model(gbm, X_test, y_test)\n",
    "        logging.info(f\"Unlearned Model - Accuracy: {acc_unlearn:.4f}, Precision: {prec_unlearn:.4f}, Recall: {rec_unlearn:.4f}, F1: {f1_unlearn:.4f}, Log Loss: {ll_unlearn:.4f}\")\n",
    "\n",
    "        # Retrain a baseline model without the removed data\n",
    "        logging.info(\"Retraining the baseline model without the removed samples...\")\n",
    "        X_train_retrained = X_train.drop(index=indices_to_remove).reset_index(drop=True)\n",
    "        y_train_retrained = y_train.drop(index=indices_to_remove).reset_index(drop=True)\n",
    "        baseline_model = GradientBoostedTrees(n_estimators=10, learning_rate=0.1, max_depth=3, min_samples_split=10)\n",
    "        baseline_model.fit(X_train_retrained, y_train_retrained)\n",
    "\n",
    "        # Evaluate the baseline model\n",
    "        acc_base, prec_base, rec_base, f1_base, ll_base = evaluate_model(baseline_model, X_test, y_test)\n",
    "        logging.info(f\"Retrained Model - Accuracy: {acc_base:.4f}, Precision: {prec_base:.4f}, Recall: {rec_base:.4f}, F1: {f1_base:.4f}, Log Loss: {ll_base:.4f}\")\n",
    "\n",
    "        # Compare Unlearned Model with Retrained Model\n",
    "        logging.info(\"Comparison between Unlearned Model and Retrained Model:\")\n",
    "        logging.info(f\"Accuracy Difference: {abs(acc_unlearn - acc_base):.4f}\")\n",
    "        logging.info(f\"Precision Difference: {abs(prec_unlearn - prec_base):.4f}\")\n",
    "        logging.info(f\"Recall Difference: {abs(rec_unlearn - rec_base):.4f}\")\n",
    "        logging.info(f\"F1 Score Difference: {abs(f1_unlearn - f1_base):.4f}\")\n",
    "        logging.info(f\"Log Loss Difference: {abs(ll_unlearn - ll_base):.4f}\")\n",
    "\n",
    "        # Optional: Implement membership inference tests here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdef924-7995-488d-a464-031ff5f4fecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.4xlarge",
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
